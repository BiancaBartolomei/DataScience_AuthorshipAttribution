{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFG_Análise Léxica.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r89GmUvJHs6C"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwJoTq_DHYk5"
      },
      "source": [
        "# CARREGAR ARQUIVOS\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def read_folder(folder_path, texts_list):\n",
        "  all_files = os.listdir(folder_path)\n",
        "\n",
        "  for file_name in all_files:\n",
        "    file = open(folder_path+file_name, 'r+', encoding = \"ISO-8859-1\")\n",
        "    text_content = file.read()\n",
        "    texts_list.append((text_content, file_name))\n",
        "    file.close()\n",
        "\n",
        "   \n",
        "folder_path = '/content/drive/My Drive/Resumos/'\n",
        "texts_list = []\n",
        "read_folder(folder_path, texts_list)\n",
        "\n",
        "corpus_df = pd.DataFrame(texts_list, columns = ['text', 'file_name']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atmHz8yjxWXB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2a54bbfa-87b7-4616-b338-ae19c87bf44d"
      },
      "source": [
        "import time\n",
        "\n",
        "start_1 = time.time()\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    doc = re.sub(\"www.\\S+\",\"\", doc)\n",
        "    doc = re.sub(\"\\n\", \" \", doc)\n",
        "    doc = re.sub('-se', ' se', doc)\n",
        "    doc = re.sub('-a', ' a', doc)\n",
        "    doc = re.sub('-o', ' o', doc)\n",
        "    doc = re.sub('-lhe', ' lhe', doc)\n",
        "    doc = re.sub('-la', ' la', doc)\n",
        "    doc = re.sub('-lo', ' lo', doc)\n",
        "    doc = re.sub('-me', ' me', doc)\n",
        "    doc = re.sub('-', ' ', doc)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(corpus_df['text'])\n",
        "\n",
        "from string import punctuation\n",
        "remove_terms = punctuation + '0123456789'\n",
        "\n",
        "norm_corpus = [[word.lower() for word in sent if word not in remove_terms] for sent in norm_corpus]\n",
        "norm_corpus = [''.join(tok_sent) for tok_sent in norm_corpus]\n",
        "norm_corpus = [tok_sent for tok_sent in norm_corpus if len(tok_sent.split()) > 2]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0.5, max_df=1., use_idf=True, strip_accents='ascii', stop_words=stop_words)\n",
        "tv_matrix = tv.fit_transform(corpus_df['text'])\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "norm_corpus_pd = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab, index=corpus_df['file_name'])\n",
        "\n",
        "print(norm_corpus_pd.shape)\n",
        "norm_corpus_pd.to_csv('entrada_lexica.csv')\n",
        "\n",
        "\n",
        "end_1 = time.time()\n",
        "\n",
        "\n",
        "print(\"Time: \" + str((end_1 - start_1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "(90, 33)\n",
            "Time: 0.37558817863464355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8oZcp6exZBw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "59f50dc5-a73c-4c1c-ae7f-71d817dc23a4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0.5, max_df=1., use_idf=True, strip_accents='ascii', stop_words=stop_words)\n",
        "tv_matrix = tv.fit_transform(corpus_df['text'])\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "norm_corpus_pd = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab, index=corpus_df['file_name'])\n",
        "\n",
        "print(norm_corpus_pd.shape)\n",
        "norm_corpus_pd.to_csv('entrada_lexica.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}